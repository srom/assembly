"""
Create DB subset (`db_proka small`).

Input:
- Path to current DB folder
- Metadata CSV file (can be generated with `GTDB_r220_subset.py`)
- Path to output DB folder

Output: same as main DB but for only for the provided subset of assembly accessions.
"""
import argparse
import gzip
import logging
from pathlib import Path
from multiprocessing import Process
import shutil
import subprocess
import sys
import tempfile

import numpy as np
import pandas as pd
from Bio import Phylo, SeqIO

from src.utils import get_accession_from_path_name, get_n_cpus
from src.db_proka.make_tree_subset import prune_leaves_with_unknown_id


logger = logging.getLogger()


def main():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s %(processName)-10s (%(levelname)s) %(message)s')

    parser = argparse.ArgumentParser(
        description='Create subset of main DB',
    )
    parser.add_argument(
        '-i', '--base_folder', 
        help='Path to base folder containing genome assemblies, as generated by the script fetch_assemblies.py', 
        type=Path,
        required=True,
    )
    parser.add_argument(
        '-m', '--metadata_path', 
        help='Path to GTDB metadata file containing assemblies to be processed.', 
        required=True,
        type=Path,
    )
    parser.add_argument(
        '-o', '--output_folder', 
        help='Path to small DB folder', 
        type=Path,
        required=True,
    )
    parser.add_argument('--cpu', type=int, default=4)
    args = parser.parse_args()

    base_folder = args.base_folder
    metadata_path = args.metadata_path
    output_folder = args.output_folder
    n_cpus = min(args.cpu, get_n_cpus())

    genomes_folder = base_folder / 'genomes'

    if not base_folder.is_dir():
        logger.error(f'Assemblies folder does not exist: {args.assemblies}')
        sys.exit(1)
    elif not genomes_folder.is_dir():
        logger.error(f'Genomes folder does not exist: {genomes_folder}')
        sys.exit(1)
    elif not metadata_path.is_file():
        logger.error(f'Metadata file does not exist: {metadata_path}')
        sys.exit(1)
    elif not output_folder.is_dir():
        logger.error(f'Output directory does not exists: {args.output_path}')
        sys.exit(1)

    logger.info('Loading metadata')
    metadata_df = pd.read_csv(metadata_path, index_col='assembly_accession')
    accessions = set(metadata_df.index)

    logger.info('Finding relevant assembly folders')
    paths = sorted([
        p for p in genomes_folder.iterdir()
        if (
            p.is_dir() and 
            p.name.startswith('GC') and 
            get_accession_from_path_name(p) in accessions
        )
    ])

    logger.info(f'Total number of assemblies: {len(paths):,}')

    if len(paths) == 0:
        sys.exit(0)
    
    logger.info('Copying genomes to smaller DB folder')
    copy_genomes(paths, output_folder, n_cpus)

    logger.info('Create subset of summary files')
    file_types_to_process = [
        ('_all_proteins.fasta', 'fasta'),
        ('_Pfam-A_hits.csv', 'csv'),
        ('_TIGR_hits.csv', 'csv'),
        ('_Pfam-A_summary.tsv.gz', 'tsv'),
        ('_TIGR_summary.tsv.gz', 'tsv'),
        ('_tree_ar53.phyloxml', 'tree'),
        ('_tree_bac120.phyloxml', 'tree'),
    ]
    make_subset_of_files(base_folder, file_types_to_process, output_folder, accessions)

    logger.info('DONE')
    sys.exit(0)


def copy_genomes(paths, output_folder, n_cpus):
    target_folder = output_folder / 'genomes'
    target_folder.mkdir(exist_ok=True)

    def copy_genome_worker(worker_ix, paths, target_folder):
        logging.basicConfig(level=logging.INFO, format='%(asctime)s %(processName)-10s (%(levelname)s) %(message)s')
        logger.info(f'Copy genome worker {worker_ix+1} is starting')

        for i, path in enumerate(paths):
            if i == 0 or (i+1) % 100 == 0 or (i+1) == len(paths):
                logger.info(f'Worker {worker_ix+1} | Processing assembly {i+1:,} / {len(paths):,}')

            destination_folder = target_folder / path.name
            shutil.copytree(path, destination_folder, dirs_exist_ok=True)

        logger.info(f'Copy genome worker {worker_ix+1} is done')

    n_processes = min(n_cpus, len(paths))
    n_per_process = int(np.ceil(len(paths) / n_processes))

    processes = []
    for i in range(n_processes):
        start = i * n_per_process
        end = start + n_per_process
 
        p = Process(target=copy_genome_worker, args=(
            i,
            paths[start:end],
            target_folder,
        ))
        p.start()
        processes.append(p)

    for p in processes:
        p.join()


def make_subset_of_files(base_folder, file_types_to_process, output_folder, accessions):
    for (file_suffix, file_type) in  file_types_to_process:
        filepath = None
        for f in base_folder.iterdir():
            if f.name.endswith(file_suffix):
                filepath = f
                break

        if filepath is None:
            logger.error(f'No file found for file suffix: {file_suffix}')
            continue

        output_path = output_folder / f'{output_folder.name}{file_suffix}'

        if output_path.is_file():
            logger.error(f'File already exists: {output_path}')
            continue

        logger.info(f'Processing and exporting to {output_path}')

        if file_type in ('csv', 'tsv'):
            separator = ',' if file_type == 'csv' else '\t'
            make_table_subset(filepath, output_path, accessions, separator)
        elif file_type == 'fasta':
            make_fasta_subset(filepath, output_path, accessions)
        elif file_type == 'tree':
            make_tree_subset(filepath, output_path, accessions)
        else:
            raise ValueError(f'Unsupported file type: {file_type}')


def make_table_subset(input_path, output_path, accessions, separator):
    chunk_reader = pd.read_csv(
        input_path, 
        sep=separator, 
        index_col='assembly_accession',
        chunksize=int(1e5),
    )
    with tempfile.NamedTemporaryFile(delete=False) as temp_f:
        temp_path = Path(temp_f.name).resolve()

    try:
        header = True
        for df in chunk_reader:
            intersection = accessions & set(df.index)
            if len(intersection) > 0:
                df.loc[sorted(intersection)].to_csv(
                    temp_path,
                    sep=separator,
                    header=header,
                    mode='a',
                )
                header = False

        write_final_output(temp_path, output_path)
        
    finally:
        if temp_path.is_file():
            temp_path.unlink()


def make_fasta_subset(input_path, output_path, accessions):
    def process_fasta(f_in, output_path, accessions, write_every=int(1e5)):
        outputs = []
        for record in SeqIO.parse(f_in, 'fasta'):
            assembly_accession = record.id.split('@')[1].split('$')[0]
            if assembly_accession in accessions:
                outputs.append(record)

            if len(outputs) >= write_every:
                with output_path.open('a') as f_out:
                    SeqIO.write(outputs, f_out, 'fasta')
                outputs = []

        if len(outputs) >= write_every:
            with output_path.open('w') as f_out:
                SeqIO.write(outputs, f_out, 'fasta')
        return

    with tempfile.NamedTemporaryFile(delete=False, suffix='.fasta') as temp_f:
        temp_path = Path(temp_f.name).resolve()

    try:
        if input_path.name.endswith('gz'):
            with gzip.open(input_path, 'rt') as f_in:
                process_fasta(f_in, temp_path, accessions)
        else:
            with input_path.open('rt') as f_in:
                process_fasta(f_in, temp_path, accessions)

        write_final_output(temp_path, output_path)

    finally:
        if temp_path.is_file():
            temp_path.unlink()


def make_tree_subset(input_path, output_path, accessions):
    tree = Phylo.read(input_path, 'phyloxml')
    tree_subset = prune_leaves_with_unknown_id(tree, accessions)
    with output_path.open('w') as f_out:
        Phylo.write([tree_subset], f_out, 'phyloxml')


def write_final_output(temp_path, output_path):
    if output_path.name.endswith('.gz'):
        # Compress temp file to final location
        with output_path.open('wb') as f_out:
            response = subprocess.run(
                ['gzip', '-c', temp_path.resolve().as_posix()], 
                stdout=f_out,
            )
        if response.returncode != 0:
            logger.error(f'Error while compressing CSV output {output_path}')
            return
    else:
        # Copy temp file to final location
        shutil.copy(temp_path, output_path)


if __name__ == '__main__':
    main()
